{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devparikh0506/DATA-602/blob/main/week_7/Homework_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd-BTPFD4MBH"
      },
      "source": [
        "# Preamble\n",
        "This problem set is an extension of Problem Set 6.  You will need the MNIST 784 dataset from OpenML, with dimensionality reduced to about 75\\% of original variance.\n",
        "\n",
        "As with last week, the first 60,000 observations are available to use as training data, and the remaining 10,000 images as test data.  In training the models, you do not need to use all 60,000 observations.  (It is suggested to partition the training data into a training dataset and holdout dataset rather than use cross-validation.  Training on as few as 5000 observations is sufficient to reduce training time.)\n",
        "\n",
        "For purposes of this problem set, recode the target variable for both the test and training sets to classify whether a digit is less than 5 (i.e., $y \\in \\left\\{0, 1, 2, 3, 4\\right\\}$).  That is, the target variable should take the value 0 where the corresponding observation depicts a 0, 1, 2, 3, or 4; and the value 1 where the corresponding observation depicts a 5, 6, 7, 8, or 9.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "lg8cxlS2PevV"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "IJwiByZP_8aN"
      },
      "outputs": [],
      "source": [
        "#fetch OpenML data\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
        "\n",
        "#split into test/training sets\n",
        "train_bound = slice(0, 60_000)\n",
        "test_bound = slice(60_000, None)\n",
        "partition = lambda X, y, slc: (X[slc], y[slc])\n",
        "X_train_full, y_train_full = partition(X, y, train_bound)\n",
        "X_test, y_test = partition(X, y, test_bound)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "I0xq5028QMbG"
      },
      "outputs": [],
      "source": [
        "X_train, X_holdout, y_train, y_holdout = train_test_split(\n",
        "    X_train_full, y_train_full, train_size=5000, random_state=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "A4VwrsbFRUr8"
      },
      "outputs": [],
      "source": [
        "# Reference taken from week-6 homework\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Since we know the min and max bounds for training data its good to use MinMaxScaler\n",
        "mm_scaler = MinMaxScaler()\n",
        "\n",
        "# For PCA using n_components equals to 0.75 to take 75% of original variance\n",
        "pca = PCA(n_components=0.75)\n",
        "\n",
        "# Using pipeline to apply scaler and PCA\n",
        "pipe = Pipeline(steps=[('pca', pca)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRDWZukSReO-",
        "outputId": "c5ed41bf-9b32-4367-ce7a-f818ab2d2a57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of training dataset before PCA: (5000, 784)\n",
            "shape of training dataset after PCA: (5000, 33)\n"
          ]
        }
      ],
      "source": [
        "X_train_pca = pipe.fit_transform(X_train)\n",
        "print(f\"shape of training dataset before PCA: {X_train.shape}\")\n",
        "print(f\"shape of training dataset after PCA: {X_train_pca.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "Khz9JBm-SEyk"
      },
      "outputs": [],
      "source": [
        "X_test_pca = pipe.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjOfKf7ZRq0d"
      },
      "source": [
        "Let's convert Y_train and Y_test to 0 or 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "dtnm6MlSRosE"
      },
      "outputs": [],
      "source": [
        "y_train_final = (y_train.astype(int) >= 5).astype(int)\n",
        "y_test_final = (y_test.astype(int) >= 5).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tPtoB1Y3v77"
      },
      "source": [
        "# Problem 1 -- Classifiers\n",
        "\n",
        "Train 3 classifiers on the dataset, each using a different algorithm.  Each classifier must have an $F_1$ score of at least 0.9.  At least one classifier must use gradient boosting (AdaBoost, Gradient Boost, or xgboost).  Show the $F_1$ score and classification report for each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "4IntFJG53sOk"
      },
      "outputs": [],
      "source": [
        "scores={}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "5_xMitMEVUxP"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EDRUrBrUpAz",
        "outputId": "8ad553b1-24e9-4307-f5be-ae180ed0bdd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 2 folds for each of 27 candidates, totalling 54 fits\n",
            "Fitting 2 folds for each of 27 candidates, totalling 54 fits\n",
            "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Gradient Boosting Results:\n",
            "Best parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}\n",
            "F1 Score: 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      2564\n",
            "           1       1.00      1.00      1.00      2436\n",
            "\n",
            "    accuracy                           1.00      5000\n",
            "   macro avg       1.00      1.00      1.00      5000\n",
            "weighted avg       1.00      1.00      1.00      5000\n",
            "\n",
            "\n",
            "Random Forest Results:\n",
            "Best parameters: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 200}\n",
            "F1 Score: 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      2564\n",
            "           1       1.00      1.00      1.00      2436\n",
            "\n",
            "    accuracy                           1.00      5000\n",
            "   macro avg       1.00      1.00      1.00      5000\n",
            "weighted avg       1.00      1.00      1.00      5000\n",
            "\n",
            "\n",
            "ADABoost Results:\n",
            "Best parameters: {'learning_rate': 1.0, 'n_estimators': 200}\n",
            "F1 Score: 0.8917819365337672\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.89      0.90      2564\n",
            "           1       0.88      0.90      0.89      2436\n",
            "\n",
            "    accuracy                           0.89      5000\n",
            "   macro avg       0.89      0.89      0.89      5000\n",
            "weighted avg       0.89      0.89      0.89      5000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def get_optimized_classifier(classifier, param_grid, X_train, y_train):\n",
        "    \"\"\"\n",
        "    Perform GridSearchCV for a given classifier and return the optimized model along with its performance metrics.\n",
        "\n",
        "    Args:\n",
        "    classifier: The classifier object to be optimized\n",
        "    param_grid (dict): Parameter grid for GridSearchCV\n",
        "    X_train, y_train: Training data\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary containing the best model, its parameters, F1 score, and classification report\n",
        "    \"\"\"\n",
        "\n",
        "    grid_search = GridSearchCV(classifier, param_grid, cv=2, scoring='f1', n_jobs=-1, verbose=2)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "    # Not using test data at the moment, we'll just evaluate performance based on training data\n",
        "    y_pred = best_model.predict(X_train)\n",
        "    f1 = f1_score(y_train, y_pred)\n",
        "\n",
        "    return {\n",
        "        'model': best_model,\n",
        "        'best_params': grid_search.best_params_,\n",
        "        'f1_score': f1,\n",
        "        'classification_report': classification_report(y_train, y_pred)\n",
        "    }\n",
        "\n",
        "classifiers = [\n",
        "    (GradientBoostingClassifier(), {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7]\n",
        "    }),\n",
        "    (RandomForestClassifier(), {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [5, 10, None],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    }),\n",
        "    (AdaBoostClassifier(), {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 1.0]\n",
        "    })\n",
        "]\n",
        "\n",
        "\n",
        "results = []\n",
        "for classifier, param_grid in classifiers:\n",
        "    result = get_optimized_classifier(classifier, param_grid, X_train_pca, y_train_final)\n",
        "    results.append(result)\n",
        "\n",
        "\n",
        "for clf_name, result in zip(['Gradient Boosting', 'Random Forest', 'ADABoost'], results):\n",
        "    print(f\"\\n{clf_name} Results:\")\n",
        "    print(f\"Best parameters: {result['best_params']}\")\n",
        "    print(f\"F1 Score: {result['f1_score']}\")\n",
        "    print(result['classification_report'])\n",
        "\n",
        "best_models = [result['model'] for result in results]\n",
        "best_grd, best_rf, best_ada = best_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG5qDkzu4uRS"
      },
      "source": [
        "# Problem 2 -- Voting ensemble model\n",
        "\n",
        "(20 pts) Build a voting ensemble model that combines the three classifiers from the previous problem, in addition to the SVM model developed last week.  What is the $F_1$ score of the ensemble model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeXyMBsUdaFM"
      },
      "source": [
        "Let's create classifier from hyperparameters we found last week"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "QIReirN0418z"
      },
      "outputs": [],
      "source": [
        "# Using hyperparameters from last week's assignment to get best SVM model\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "best_svc = SVC(kernel='rbf', C=10)\n",
        "svc_pipeline = make_pipeline(mm_scaler, best_svc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGgllWezfJQU",
        "outputId": "e3aa2079-62c6-439e-f671-1f2fce41225d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score of voting ensemble model: 1.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "clfs = [('ada', best_ada),\n",
        "        ('grd', best_grd),\n",
        "        ('rf', best_rf),\n",
        "        ('svc', svc_pipeline)]\n",
        "\n",
        "clf_vot = VotingClassifier(clfs)\n",
        "\n",
        "# fit training data\n",
        "clf_vot.fit(X_train_pca, y_train_final)\n",
        "\n",
        "# Evaluating f1 score based on training data\n",
        "y_pred = clf_vot.predict(X_train_pca)\n",
        "f1 = f1_score(y_train_final, y_pred)\n",
        "print(f\"F1 score of voting ensemble model: {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2Pnq0i042SD"
      },
      "source": [
        "## Problem 3 -- Stacking ensemble model\n",
        "Stacking uses a final classifier (often a logistic regression) that outputs an aggregate of the predictors. Repeat the previous problem using a StackingClassifier rather than voting to compute the final prediction.  What is the $F_1$ score of the stacking classifier?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "cQQ6Y1zG6K3m"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zx1wbMGBiIr9",
        "outputId": "d2cf2405-6907-4aef-a6e9-25ef64ca463d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score of stacking ensemble model: 1.0\n"
          ]
        }
      ],
      "source": [
        "stacking_clf = StackingClassifier(\n",
        "    estimators=clfs,\n",
        "    final_estimator=LogisticRegression(),\n",
        "    verbose=1\n",
        ")\n",
        "stacking_clf.fit(X_train_pca, y_train_final)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_stacking = stacking_clf.predict(X_train_pca)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1_stacking = f1_score(y_train_final, y_pred_stacking)\n",
        "\n",
        "print(f\"F1 score of stacking ensemble model: {f1_stacking}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}